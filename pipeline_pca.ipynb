{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Gender Classification\n",
    "### Final Project\n",
    "\n",
    "#### University of California, Santa Barbara\n",
    "#### PSTAT 135: Big Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/crowdflower/twitter-user-gender-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains about 20,000 rows, each with a user name, a random tweet, profile image and statistics, location, and link and sidebar color. All tweets were posted in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "tweets = spark.read.csv('gender_data.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24230"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unit_id',\n",
       " '_golden',\n",
       " '_unit_state',\n",
       " '_trusted_judgments',\n",
       " '_last_judgment_at',\n",
       " 'gender',\n",
       " 'gender:confidence',\n",
       " 'profile_yn',\n",
       " 'profile_yn:confidence',\n",
       " 'created',\n",
       " 'description',\n",
       " 'fav_number',\n",
       " 'gender_gold',\n",
       " 'link_color',\n",
       " 'name',\n",
       " 'profile_yn_gold',\n",
       " 'profileimage',\n",
       " 'retweet_count',\n",
       " 'sidebar_color',\n",
       " 'text',\n",
       " 'tweet_coord',\n",
       " 'tweet_count',\n",
       " 'tweet_created',\n",
       " 'tweet_id',\n",
       " 'tweet_location',\n",
       " 'user_timezone']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove columns `_unit_id`, `_golden`, `_unit_state`, `_last_judgment_at`,`_trusted_judgments`, `gender:confidence`, `profile_yn`, `profile_yn:confidence`, `description`, `gender_gold`, `link_color`,`name`, `profile_yn_gold`, `profileimage`, `sidebar_color`, `tweet_coord`, `tweet_id`, `tweet_location`, and `user_timezone` because they were not relevant in our model/purpose...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+-------------+--------------------+-----------+--------------+\n",
      "|gender|       created|fav_number|retweet_count|                text|tweet_count| tweet_created|\n",
      "+------+--------------+----------+-------------+--------------------+-----------+--------------+\n",
      "|  male|  12/5/13 1:48|         0|            0|Robbie E Responds...|     110964|10/26/15 12:40|\n",
      "|  male| 10/1/12 13:51|        68|            0|���It felt like t...|       7471|10/26/15 12:40|\n",
      "|  male|11/28/14 11:30|      7696|            1|i absolutely ador...|       5617|10/26/15 12:40|\n",
      "|  male| 6/11/09 22:39|       202|            0|Hi @JordanSpieth ...|       1693|10/26/15 12:40|\n",
      "|female| 4/16/14 13:23|     37318|            0|Watching Neighbou...|      31462|10/26/15 12:40|\n",
      "+------+--------------+----------+-------------+--------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.select(['gender','created','fav_number','retweet_count','text','tweet_count','tweet_created'])\n",
    "tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              gender|count|\n",
      "+--------------------+-----+\n",
      "|              female| 6700|\n",
      "|                male| 6194|\n",
      "|               brand| 5942|\n",
      "|                null| 3188|\n",
      "|             unknown| 1117|\n",
      "|          6.5874E+17|   83|\n",
      "|          6.5873E+17|   62|\n",
      "|              Medina|   14|\n",
      "|              London|   13|\n",
      "|      10/26/15 12:40|   11|\n",
      "|Porto, Portugal �...|   10|\n",
      "|1/16 cute pickle ...|   10|\n",
      "|              0084B4|    9|\n",
      "|Republic of the P...|    8|\n",
      "|                  UK|    7|\n",
      "|                 USA|    7|\n",
      "|         3/4 + band |    6|\n",
      "|              FF005E|    6|\n",
      "|PRE-ORDER MADE IN...|    5|\n",
      "|           New York |    4|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.groupBy('gender').count().sort('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the `gender` column contains many labels. We decide to keep only the tweets made by a `male`, `female`, or `brand` since they are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18836"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.filter((tweets.gender == 'male') | (tweets.gender == 'female') | (tweets.gender == 'brand'))\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove any null values in `text` since we are only interested in predicting gender for users with tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of null values in \"text\"\n",
    "tweets.filter(tweets.text.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17748"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.filter(tweets.text.isNotNull())\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns `fav_number`, `retweet_count`, and `tweet_count`:\n",
    "\n",
    "- If more than 10% of the values are null, we will not include that column later on in our model. \n",
    "\n",
    "- If less than 10% of the values are null, we will replace null values with the median of that specific column. We chose median because there is a possibility the mean could be something like 77.5, and you cannot retweet something 77.5 times.\n",
    "\n",
    "- If there are no null values, we will keep the column unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of null values in \"fav_number\"\n",
    "tweets.filter(tweets.fav_number.isNull()).count()/tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of null values in \"retweet_count\"\n",
    "tweets.filter(tweets.retweet_count.isNull()).count()/tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07093757043047104"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of null values in \"tweet_count\"\n",
    "tweets.filter(tweets.tweet_count.isNull()).count()/tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since less than 10% of `tweet_count` values are null, we will replace them with the median value of `tweet_count` during pipeline building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|female| 6354|\n",
      "|  male| 5776|\n",
      "| brand| 5618|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.groupBy('gender').count().orderBy('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pie chart of genders\n",
    "import matplotlib.pyplot as plt\n",
    "labels = 'Brand', 'Female', 'Male'\n",
    "sizes = [5618, 6354, 5776]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90, colors=['lightgreen', 'hotpink', 'deepskyblue'])\n",
    "ax1.axis('equal')\n",
    "plt.title('Gender Distribution of Users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------------+-------------------+\n",
      "|gender| avg(tweet_count)|  avg(fav_number)| avg(retweet_count)|\n",
      "+------+-----------------+-----------------+-------------------+\n",
      "|female|26868.96356206368|6039.206956248033|0.04737173434057287|\n",
      "| brand| 63219.3443287037|2086.142043431826|0.11765752936988252|\n",
      "|  male|30534.52625698324|4929.307825484764|0.09279778393351801|\n",
      "+------+-----------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.groupBy('gender') \\\n",
    "    .agg({'fav_number':'avg','retweet_count':'avg','tweet_count':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar Plot of average number of tweets per account by gender\n",
    "tweet_count_x = ['Brand', 'Female', 'Male']\n",
    "tweet_count_avg = [63219.344329, 26868.963562, 30534.526257]\n",
    "x_pos = [i for i, _ in enumerate(tweet_count_x)]\n",
    "plt.bar(tweet_count_x, tweet_count_avg, color=['limegreen','hotpink','deepskyblue'])\n",
    "plt.ylabel(\"Average\")\n",
    "plt.title(\"Average Number Of Tweets Per Account \\n (by gender)\")\n",
    "plt.xticks(x_pos, tweet_count_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar Plot of average number of favorites per account by gender\n",
    "fav_number_x = ['Brand', 'Female', 'Male']\n",
    "fav_number_avg = [2086.142043431826, 6039.206956248033, 4929.307825484764]\n",
    "x_pos = [i for i, _ in enumerate(fav_number_x)]\n",
    "plt.bar(fav_number_x, fav_number_avg, color=['limegreen','hotpink','deepskyblue'])\n",
    "plt.ylabel(\"Average\")\n",
    "plt.title(\"Average Number Of Favorited Tweets Per Account \\n (by gender)\")\n",
    "plt.xticks(x_pos, fav_number_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEXCAYAAAC3c9OwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de9wVZb338c9XEFHDKCEPgEJJGfoyNUIs22kesQwtK8087Ypsa2pP7h631Y7S2tWutrpzS6ZWeC7NYht7az6p5ZmDiCFqeAoUFM94RPT3/HFdS4bFdXOvGxgW3Pf3/Xqt1z0z1zUzv1lr7vVbc83MNYoIzMzMmq3X7gDMzGzt5ARhZmZFThBmZlbkBGFmZkVOEGZmVuQEYWZmRU4Qts6QNF7ShWtBHF+S9Jik5yVt2u54zOriBLEGSbpe0tOSNmh3LKtD3p6XJQ2pTNtL0kNtDGuVSHq/pD9JWiTpWUn/LWlEpXx94CfAPhHxpoh4smn+oZIiJ4/nJT0k6eQurH+NJ8FKzL1XYd6V2t4urOeXkpZI2nJ1L3t1yu/FNu2OY3VxglhDJA0FPggE8LGa1tHlf/DV4AXgm21Y7yopvVeSdgWuAX4PbAkMA+4EbpL09lxtM6AvMKuTVfSPiDcBBwPflLT36op9LdXY3kOBf5W0X1cXIKlXB9M3Bj4BPAsctkpRWpc4Qaw5RwC3Ar8EjmxMlDRa0oLqP4ekgyTNzMPrSTpZ0v2SnpT0a0lvzWWNX2+fk/R34E95+m/yMp+V9GdJ21WWvWn+VfycpCmSTpN0Y6V8W0l/lPSUpHslfaqT7ToTOLSjX03Nv6jyL8HT8vDukuZJ+pqkxyXNl3SgpP0l3ZdjOKVpkX0lXZZ/4U+X9J7KsreUdIWkhZIelHR8pWy8pMslXSjpOeCoQrg/BCZGxBkRsSginoqIb5A+t/GS3gncm+s+I+lPnbw3RMRUUjLZsbM485fqKcCn86/xOyXtIemuyrzXSrq9Mn6jpANb2P4O9yPgz5Vtel7SrpK2kXRD3oeekHRZZ9uat/eWvL3b5/V2uD/lfeFsSZMlvQDs0cFiPwE8A3yHyv9OXkYvSafk7VokaZryEa2k7SrrfqyxL0naQNLpkh7Nr9OVj+olHVX9f8jT3tiHc8xnSfpDXt9tkt6Ryxrv4535ffx0K+/ZWi0i/FoDL2AO8E/Ae4FXgc0qZfcDe1fGfwOcnIdPJH1BDQY2AH4GXJLLhpKOSCYCGwMb5un/CPTL9U8HZlSWfWl+bQSMAOYCN+ayjfP40UBvYGfgCWC7DrbpeuDzpCaXC/O0vYCHKnUC2KYy/kvgtDy8O7AE+FdgfeALwELg4hz/dsDLwNtz/fH5vTs41z8JeDAPrwdMy8vqA7wdeADYt2neA3PdDZu2ZSPgNWCPwnYeDcxves97d/CeLFMOjAZeBA7K463EeWFleX2Bl4AB+TNZADya358Nc9mmLSy3lf2od2W9lwBfz8vtC+zW2fYCAj6Qt3dPOtmf8r7wbJ5nPaBvB+v4f6TkvRlpf9m5UvbPwF3Au/L635Pfj37AfOCrOf5+wC55nu/k9+JtwEDgZuDUXHYU+f+htA/nmJ8CRuVtugi4tKP9fV1/tT2AnvACdiN9OQ3I4/cAX6mUnwacn4f7kZptts7js4E9K3W3yMvqXfnnfPsK1t0/13kz0CvP+66mdTcSxKeBvzTN/zPgWx0s+3pSghiY/9G3o+sJ4iWgV2Xbo/GPnKdNAw7Mw+OBWytl6+UvgQ8CuwB/b4rvX4BfVOb98wrep8F53dsWyvYDXs3Djfe8swTxTN62AH4EKJe3EueFTeV/AT5OSjbXAL/OMe0BzGxxua3sR9UEMRE4Bxjcyb5d3d6n83qOb2V/yvvCxE6WvxXwOrBjHr8aOKNSfi8wtjDfocAdHSzzfmD/yvi+5H2W1hLEuZWy/YF7Otrf1/VXO9qse6IjgWsi4ok8fnGe9h+V8ZslfYn0RTA9Ih7OZVsDV0p6vbK810i/phrmNgaUmqq+C3yS9MXdmG8A6Rdn72r9puGtgV0kPVOZ1hu4YEUbFxELJf2U9Mvs7BXVLXgyIl7Lwy/lv49Vyl8C3lSKNyJelzSPdL4ggC2bYu9F+nJdbt6Cp0nv1RakBF61BemXb1cMyDGdSPqyWh9YTHqPO4uz2Q2kZDovDz8NfAh4JY/TwnJb2Y+qvgacCtwu6WngxxFx/gpiHBARS5qmtbI/regzATgcmB0RM/L4RcCPJZ0UEa8CQ0hf+M06mg5pf3m4Mv5wntaqBZXhF1l2/+xWnCBqJmlD4FNAL0mNHWsDoL+k90TEnRFxt6SHgTHAZ0gJo2Eu8I8RcVNh2UPzYLVL3s8AY8m/5ElHDk+TDr8Xkg7RBwP35fpDKvPOBW6IiJU5ofrvpCaN25umv0hqvmnYnPRFt7KqV0ytR9qWR0nb9WBEDF/BvB12XRwRL0i6hZRYr2sq/hSpmaNLcuL7saSDSM2Lp5Pe4xXFWYrxBuDHwN+B75M+z5+TEsRZuU5ny13RfrR1IfYFpCY/JO0GXCvpzxExp4Pld7TOzvanzrqTPgLYqvK/05vUhDQGmJTX8Q7gr4V1H9rBMh8lJa/GhQZb5WmQjt7f2F8lbd5JfN2aT1LX70DSL7URpBOVOwLvJv2yO6JS72LgeOAfSOcgGiYA3238E0saKGnsCtbXj/TF8SRpR/9eoyB/Yf2WdMJ1I0nbNsVwFfBOSYdLWj+/3ifp3Z1tZEQ8Q/oS+1pT0QzgM/lk4n6kX76r4r2SPq50FdKJpG29lZSYnpP0fyVtmNe3vaT3dWHZJwNHSjpeUj9Jb1E6ob4r8O1ViPn7wNck9W0hzseAoTn5NdxMamMfBdweEbPIv85ZeoK5s+WuaD9aSDp6alyphaRPShqcR58mfZE3jvRatdL7U45hV9KX/yiW/u9sz9IjcIBzgVMlDVeyg9K9KVcBm0s6MZ+U7idplzzPJcA38nswgHTepnFp8Z3AdpJ2zJ/X+C5u82NU3sd1nRNE/Y4ktQP/PSIWNF7AT4HDtPRyy0tIzQh/qjRFAZxB+qV0jaRFpC/DXejYRNIh8yPA3bl+1XGko4oFpEP9S0hfskTEImAf4BDSL6oFwA9IRzytOIPlv0ROAA4gtVEfBvyuxWV15Pektu2nSc0PH4+IV3PyO4D0JfIgqUnoXNK2tiQibiS1R3+cdG7jYWAn0gnav61CzH/I8X6hhTgbPw6elDQ9x/UCMB2YFRGLc/ktwMMR8Xiu09lyO9yPIuJFUrPkTZKekTQaeB9wm6Tn83wnRMSDXdno1bA/HQn8PiLuavrfOQP4qNJVWD8hnZO5BngOOI90AcIiYO/8niwA/sbSq6ROA6YCM0knuKfnaUTEfaSm0mvzPMtc0dSC8cCv8vvY2RWAa73GiTProST9ANg8Io7stLKZ9Sg+guhhlK5L3yEfjo8CPgdc2e64zGzt45PUPU8/UrPSlsDjpPMGv29rRGa2VnITk5mZFbmJyczMipwgrMeQdImW9lu0XJ876wKl3lL3Wsl5N5M0W92kN2GrnxOE9QiSdiD109Njz7dExGOkmwDHtTsWWzc4QVhP8UXgouihJ90q99tcRHovzDrlBGE9xRiW9lvUIEn/qdSl9T2S9swTPylpWlPFr0oq3uQnaZhSt+qLlLrjPkuVh/4odel+c7556k5Ju1fKrpd0qqSb8vzX5Lt7G+WHS3pYqYvurzett8tdwQO3AW9XoXsNs2ZOENbtKT1wZhhLn+XQsAup/6gBwLeA3+Yv2EnAsKYuIT5Lx50WXkzq6mJT0p20h1fWPYh0J/VpwFtJXZRfIWlgZf7PkLrEfhupq+6T8rwjSJ0fHk66LHlTUt9TDceTunL5UC5/mqV9MzV8iNS1y74AuUO9OaTmNrMVcoKwnqB//ruoafrjwOm5q47LSAnkIxHxCnAZKSmg9MCloaT+fZYhaStStxT/GhGLc3cdkypVPgtMjojJEfF6RPyR1M3D/pU6v4iI+yLiJVK3EY2HCx0MXBURf84xfZOlvfNCair6ekTMy+XjgYO17NPyxkfEC3nZDYsq74lZh5wgrCdodDfdr2n6I03nJKrdPv+K1MmgSL/gf52/hJttCTyV+zNqaO5C/ZO5eekZpa6vdyN1Id7QUffRW7Js9+YvkDphrC77yspyZ7OCruAr+rH0PTHrkBOEdXv5i/V+4J1NRYNyAmh4o9vniLiV9PyGD5KagDpqXpoPvFVStUvz5i7UL4iI/pXXxhHx/RZCn8+y3ZtvRGpmqi57TNOy+0bEI5U6y5yUz0cX25B6LTVbIScI6ykms3xX428Djs/dUH+S1FY/uVI+kdTr7pLcdLSc/GCnqaQu1PvkLqoPqFS5EDhA0r5KXXD3VXoW9+DS8ppcTuq1dDdJfUi9jFb/Z7vaFTykrrMfqjyQyqxDThDWU5xD6l69esRwGzCc1DX2d4GDI6LahHMB6fkDK3yiHqkb811JzT+nkc5fNLpQn0t6gNMppOcuzCU9R7nT/7383IdjSSfB55NOQlcfttTVruAbsU7obN1m4L6YrAeRdDHpXEJLz6RQehrg48DOXXkehKTLSM8p/tbKRVoPSW8jXeq7U0S83O54bO3nBGHWAUn/B/hoRHy4k3rvA54iPahnH9JDkXaNiDvqj9KsPu7u26xA0kOk53gf2EL1zUmPct2U1AT0JScH6w58BGFmZkU+SW1mZkXdqolpwIABMXTo0HaHYWa2zpg2bdoTETGwVNatEsTQoUOZOnVqu8MwM1tnSOrwnhg3MZmZWVGtCULSfpLulTRH0smF8m0l3SLpFUknVaYPkXRdfvrVLEkn1BmnmZktr7YmJkm9SF0P70269G+KpEkRcXel2lMs7bK4agnw1YiYLqkfME3SH5vmNTOzGtV5BDEKmBMRD0TEYuBSUpcDb4iIxyNiCvBq0/T5ETE9Dy8i9VI5qMZYzcysSZ0JYhDLdjU8j5X4kpc0FNiJ1G+OmZmtIXUmCBWmdemuPElvAq4AToyI5zqoM07SVElTFy5cuBJhmplZSZ0JYh7L9os/mNzXfiskrU9KDhdFxG87qhcR50TEyIgYOXBg8VJeMzNbCXUmiCnA8PxA9z7AISz7KMYO5S6ZzwNmR8RPaozRzMw6UNtVTBGxRNJxwNVAL+D8iJgl6ZhcPkHS5qSHrWwCvC7pRGAEsAPpMY93SZqRF3lKRExebkVmZlaLWu+kzl/ok5umTagMLyA1PTW7kfI5jNq8d/p71+TqepRpO09rdwhmthJ8J7WZmRU5QZiZWZEThJmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRU5QZiZWZEThJmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRU5QZiZWZEThJmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRU5QZiZWZEThJmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRU5QZiZWZEThJmZFdWaICTtJ+leSXMknVwo31bSLZJekXRSV+Y1M7N61ZYgJPUCzgLGACOAQyWNaKr2FHA88KOVmNfMzGpU5xHEKGBORDwQEYuBS4Gx1QoR8XhETAFe7eq8ZmZWrzoTxCBgbmV8Xp62WueVNE7SVElTFy5cuFKBmpnZ8upMECpMi9U9b0ScExEjI2LkwIEDWw7OzMxWrM4EMQ8YUhkfDDy6BuY1M7PVoM4EMQUYLmmYpD7AIcCkNTCvmZmtBr3rWnBELJF0HHA10As4PyJmSToml0+QtDkwFdgEeF3SicCIiHiuNG9dsZqZ2fJqSxAAETEZmNw0bUJleAGp+ailec3MbM3xndRmZlbkBGFmZkVOEGZmVuQEYWZmRU4QZmZW5ARhZmZFtV7mambWoOvbHUH3FbvXs1wfQZiZWZEThJmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRU5QZiZWZEThJmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRU5QZiZWZEThJmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRU5QZiZWVGtCULSfpLulTRH0smFckk6M5fPlLRzpewrkmZJ+qukSyT1rTNWMzNbVm0JQlIv4CxgDDACOFTSiKZqY4Dh+TUOODvPOwg4HhgZEdsDvYBD6orVzMyWV+cRxChgTkQ8EBGLgUuBsU11xgITI7kV6C9pi1zWG9hQUm9gI+DRGmM1M7MmdSaIQcDcyvi8PK3TOhHxCPAj4O/AfODZiLimtBJJ4yRNlTR14cKFqy14M7Oers4EocK0aKWOpLeQji6GAVsCG0v6bGklEXFORIyMiJEDBw5cpYDNzGypOhPEPGBIZXwwyzcTdVRnL+DBiFgYEa8CvwXeX2OsZmbWpM4EMQUYLmmYpD6kk8yTmupMAo7IVzONJjUlzSc1LY2WtJEkAXsCs2uM1czMmvSua8ERsUTSccDVpKuQzo+IWZKOyeUTgMnA/sAc4EXg6Fx2m6TLgenAEuAO4Jy6YjUzs+XVliAAImIyKQlUp02oDAdwbAfzfgv4Vp3xmZlZx3wntZmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRU5QZiZWZEThJmZFTlBmJlZUcsJQtLGdQZiZmZrl04ThKT3S7qb3BeSpPdI+q/aIzMzs7Zq5QjiP4B9gScBIuJO4B/qDMrMzNqvpSamiJjbNOm1GmIxM7O1SCud9c2V9H7Sg3z6kJ4V7a63zcy6uVaOII4h9bg6iPSAnx3poAdWMzPrPjo9goiIJ4DD1kAsZma2Fuk0QUg6szD5WWBqRPx+9YdkZmZrg1aamPqSmpX+ll87AG8FPifp9BpjMzOzNmrlJPU2wIcjYgmApLOBa4C9gbtqjM3MzNqolSOIQUD1LuqNgS0j4jXglVqiMjOztmvlCOKHwAxJ1wMi3ST3vdz1xrU1xmZmZm3UylVM50maDIwiJYhTIuLRXPzPdQZnZmbt02pnfS8D84GngG0kuasNM7NurpXLXD8PnAAMBmYAo4FbgA/XG5qZmbVTK0cQJwDvAx6OiD2AnYCFtUZlZmZt10qCeDkiXgaQtEFE3AO8q96wzMys3Vq5immepP7A74A/SnoaeLSTeczMbB3X6RFERBwUEc9ExHjgm8B5wIGtLFzSfpLulTRH0smFckk6M5fPlLRzpay/pMsl3SNptqRdW98sMzNbVSs8gpC0HjAzIrYHiIgbWl2wpF7AWaQ7rucBUyRNioi7K9XGAMPzaxfg7PwX4AzgfyPi4NzN+EatrtvMzFbdCo8gIuJ14E5JW63EskcBcyLigYhYDFwKjG2qMxaYGMmtQH9JW0jahHRD3nk5jsUR8cxKxGBmZiuplXMQWwCzJN0OvNCYGBEf62S+QUD1SXTzWHp0sKI6g4AlpCulfiHpPcA04ISIeKFpfiSNA8YBbLXVyuQxMzMraSVBfHsll63CtGixTm9gZ+DLEXGbpDOAk0nnQJatHHEOcA7AyJEjm5dvZmYrqZWuNm6QtDUwPCKulbQR0KuFZc8DhlTGB7P81U8d1QlgXkTclqdfTkoQZma2hnR6FZOkL5C+oH+WJw0iXfLamSnAcEnD8knmQ4BJTXUmAUfkq5lGA89GxPyIWEB6Fnbjfos9gbsxM7M1ppUmpmNJJ5xvA4iIv0l6W2czRcQSSccBV5OOOM6PiFmSjsnlE4DJwP7AHOBF4OjKIr4MXJSTywNNZWZmVrNWEsQrEbFYSqcLJPVm+XMJRRExmZQEqtMmVIaDlIBK884ARrayHjMzW/1a6WrjBkmnABtK2hv4DfDf9YZlZmbt1kqCOJl0yeldwBdJRwTfqDMoMzNrv1aamBo3s/287mDMzGzt0coRxMeA+yRdIOkj+RyEmZl1c6101nc0sA3p3MNngPslnVt3YGZm1l4tHQ1ExKuS/od09dJGpN5cP19nYGZm1l6t3Ci3n6RfAvcDB5O6tdi85rjMzKzNWjkHcRRwJamrjSOBRaSuuM3MrBtr5RzEIcDDwHckPQScCtxTc1xmZtZmHZ6DkPROUv9JhwJPApcBiog91lBsZmbWRis6SX0P8BfggIiYAyDpK2skKjMza7sVNTF9AlgAXCfp55L2pPz8BjMz64Y6TBARcWVEfBrYFrge+AqwmaSzJe2zhuIzM7M2aeUk9QsRcVFEfJT0QJ8Z+OE9ZmbdXiuXub4hIp6KiJ9FxIfrCsjMzNYOXUoQZmbWc7jjPVs3/dMf2h1B9/VfH2l3BLaW8BGEmZkVOUGYmVmRE4SZmRU5QZiZWZEThJmZFTlBmJlZkROEmZkVOUGYmVmRE4SZmRXVmiDy86zvlTRH0nId/Ck5M5fPlLRzU3kvSXdIuqrOOM3MbHm1JQhJvYCzgDHACOBQSSOaqo0BhufXOODspvITgNl1xWhmZh2r8whiFDAnIh6IiMXApcDYpjpjgYmR3Ar0l7QFgKTBwEeAc2uM0czMOlBnghgEzK2Mz8vTWq1zOvA14PUVrUTSOElTJU1duHDhqkVsZmZvqDNBlB5PGq3UkfRR4PGImNbZSiLinIgYGREjBw4cuDJxmplZQZ0JYh4wpDI+GHi0xTofAD4m6SFS09SHJV1YX6hmZtaszgQxBRguaZikPsAhwKSmOpOAI/LVTKOBZyNifkT8S0QMjoiheb4/RcRna4zVzMya1PbAoIhYIuk44GqgF3B+RMySdEwunwBMBvYH5gAvAkfXFY+ZmXVNrU+Ui4jJpCRQnTahMhzAsZ0s43rg+hrCMzOzFfCd1GZmVuQEYWZmRU4QZmZW5ARhZmZFThBmZlbkBGFmZkVOEGZmVuQEYWZmRU4QZmZW5ARhZmZFThBmZlbkBGFmZkVOEGZmVuQEYWZmRU4QZmZW5ARhZmZFThBmZlbkBGFmZkVOEGZmVuQEYWZmRU4QZmZW5ARhZmZFThBmZlbkBGFmZkVOEGZmVuQEYWZmRbUmCEn7SbpX0hxJJxfKJenMXD5T0s55+hBJ10maLWmWpBPqjNPMzJZXW4KQ1As4CxgDjAAOlTSiqdoYYHh+jQPOztOXAF+NiHcDo4FjC/OamVmN6jyCGAXMiYgHImIxcCkwtqnOWGBiJLcC/SVtERHzI2I6QEQsAmYDg2qM1czMmtSZIAYBcyvj81j+S77TOpKGAjsBt632CM3MrEN1JggVpkVX6kh6E3AFcGJEPFdciTRO0lRJUxcuXLjSwZqZ2bLqTBDzgCGV8cHAo63WkbQ+KTlcFBG/7WglEXFORIyMiJEDBw5cLYGbmVm9CWIKMFzSMEl9gEOASU11JgFH5KuZRgPPRsR8SQLOA2ZHxE9qjNHMzDrQu64FR8QSSccBVwO9gPMjYpakY3L5BGAysD8wB3gRODrP/gHgcOAuSTPytFMiYnJd8ZqZ2bJqSxAA+Qt9ctO0CZXhAI4tzHcj5fMTZma2hvhOajMzK3KCMDOzIicIMzMrcoIwM7MiJwgzMytygjAzsyInCDMzK3KCMDOzIicIMzMrcoIwM7MiJwgzMytygjAzsyInCDMzK3KCMDOzIicIMzMrcoIwM7MiJwgzMytygjAzsyInCDMzK3KCMDOzIicIMzMrcoIwM7MiJwgzMytygjAzsyInCDMzK3KCMDOzIicIMzMrqjVBSNpP0r2S5kg6uVAuSWfm8pmSdm51XjMzq1dtCUJSL+AsYAwwAjhU0oimamOA4fk1Dji7C/OamVmN6jyCGAXMiYgHImIxcCkwtqnOWGBiJLcC/SVt0eK8ZmZWo941LnsQMLcyPg/YpYU6g1qcFwBJ40hHHwDPS7p3FWJeVwwAnmh3EK0SancIa4N15zM7u90BrDXWmc9sFf/Dtu6ooM4EUYo5WqzTyrxpYsQ5wDldC23dJmlqRIxsdxzWOn9m6x5/ZvUmiHnAkMr4YODRFuv0aWFeMzOrUZ3nIKYAwyUNk9QHOASY1FRnEnBEvpppNPBsRMxvcV4zM6tRbUcQEbFE0nHA1UAv4PyImCXpmFw+AZgM7A/MAV4Ejl7RvHXFug7qUU1q3YQ/s3VPj//MFFFs2jczsx7Od1KbmVmRE4SZmRU5QbSRpNckzZB0p6Tpkt5f47qer2vZ3V3lc2q8hta4rqMk/bSu5fd0kkLSBZXx3pIWSrqqk/l276xOd1TnZa7WuZciYkcASfsC/wZ8qFpBUq+IeK0dwdkb3vicbJ33ArC9pA0j4iVgb+CRNse01vIRxNpjE+BpeOPXynWSLgbuytN+J2mapFn57nHy9OclfTcfhdwqabM8fZikWyRNkXRqOzaoO5PUS9K/5/d3pqQv5um7S7pB0q8l3Sfp+5IOk3S7pLskvSPXO0DSbZLukHRt43NrWsdASVfkdUyR9IE1vZ3d1P8AH8nDhwKXNAokjZJ0c/5cbpb0ruaZJW0s6fz8mdwhqft2AxQRfrXpBbwGzADuAZ4F3pun7076pTOsUvet+e+GwF+BTfN4AAfk4R8C38jDk4Aj8vCxwPPt3t519VX5nGYAV+Zp4yrv9QbAVGBY/uyeAbbI0x8Bvp3rnQCcnoffwtKrCD8P/DgPHwX8NA9fDOyWh7cCZrf7vVjXX8DzwA7A5UDf/JnuDlyVyzcBeufhvYAr8nC1zveAz+bh/sB9wMbt3rY6Xm5iaq9qE9OuwERJ2+ey2yPiwUrd4yUdlIeHkHrAfRJYDDTaRqeRDpkBPgB8Ig9fAPygnk3oEUpNTPsAO0g6OI+/mfSZLAamRLrhE0n3A9fkOncBe+ThwcBluXPKPkD1s27YCxghvdHzzCaS+kXEotWwTT1WRMzM55EOJd2LVfVm4FeShpN+fK1fWMQ+wMcknZTH+5ITeC0Bt5ETxFoiIm6RNAAYmCe90CiTtDvpy2LXiHhR0vWknRLg1cg/ZUi/dKufqW9yqY+AL0fE1ctMTJ/VK5VJr1fGX2fp5/OfwE8iYlKeZ3xhHeuRPvOXVl/Ylk0CfkQ6Mti0Mv1U4LqIOCgnkesL8wr4RER0+45BfQ5iLSFpW9Jd408Wit8MPJ2Tw7bA6BYWeROpixKAw1ZPlFZxNfAlSesDSHqnpI27MP+bWXpy9MgO6lwDHNcYkeQT5avP+cB3IuKupunVz+WoDua9Gviy8qGdpJ1qiXAt4ATRXqpNfBoAAACXSURBVBs2Lp0ELgOOjPIVS/8L9JY0k/QL59YWln0CcKykKaSd3lavc4G7gemS/gr8jK4dkY8HfiPpL3TcpfTxwMh8Evxu4JhViNcqImJeRJxRKPoh8G+SbiL9YCs5ldT0NDN/9t32IhB3tWFmZkU+gjAzsyInCDMzK3KCMDOzIicIMzMrcoIwM7MiJwgzMytygjAzs6L/Dw6E4OFlO12YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar Plot of average number of retweets per account by gender\n",
    "retweet_count_x = ['Brand', 'Female', 'Male']\n",
    "retweet_count_avg = [0.11765752936988252, 0.04737173434057287, 0.09279778393351801]\n",
    "x_pos = [i for i, _ in enumerate(retweet_count_x)]\n",
    "plt.bar(retweet_count_x, retweet_count_avg, color=['limegreen','hotpink','deepskyblue'])\n",
    "plt.ylabel(\"Average\")\n",
    "plt.title(\"Average Number Of Retweets Per Account \\n (by gender)\")\n",
    "plt.xticks(x_pos, retweet_count_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Columns to Type Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.withColumn(\"fav_number\",tweets[\"fav_number\"].cast('integer'))\n",
    "tweets = tweets.withColumn(\"retweet_count\",tweets[\"retweet_count\"].cast('integer'))\n",
    "tweets = tweets.withColumn(\"tweet_count\",tweets[\"tweet_count\"].cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Columns for Punctuation Counts, Emoji Existence, and Account Years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting punctuation counts for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "def punc_func(tweet):\n",
    "    sum = 0\n",
    "    for key in tweet:\n",
    "        if key in punctuation:\n",
    "            sum += tweet[key]\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|punc|\n",
      "+----+\n",
      "|  11|\n",
      "|  18|\n",
      "|   5|\n",
      "|  19|\n",
      "|  15|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "punc = tweets.select(\"text\").rdd.map(lambda tweet: Counter(str(tweet))) \\\n",
    "            .map(lambda x: punc_func(x)).map(lambda x: (x, )).toDF(['punc'])\n",
    "punc.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting boolean values if an emoji, represented by the character '�', is present in a tweet (True) or not present in a tweet (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|emojis|\n",
      "+------+\n",
      "| false|\n",
      "|  true|\n",
      "| false|\n",
      "| false|\n",
      "|  true|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emojis = tweets.select(\"text\").rdd.map(lambda tweet: str(tweet)) \\\n",
    "            .map(lambda x: '�' in x).map(lambda x: (x, )).toDF(['emojis'])\n",
    "emojis.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of years each Twitter user has had their account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|account_years|\n",
      "+-------------+\n",
      "|            2|\n",
      "|            3|\n",
      "|            1|\n",
      "|            6|\n",
      "|            1|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extracting only the \"year\" part of 'created' & subtracting it from 15 (representing 2015)\n",
    "years = tweets.rdd.map(lambda row: row['created'].split(' ')) \\\n",
    "                .map(lambda x: x[0]) \\\n",
    "                .map(lambda x: x.split('/')) \\\n",
    "                .map(lambda x: x[2]) \\\n",
    "                .map(lambda x: (15-int(x))) \\\n",
    "                .map(lambda x: (x, )).toDF(['account_years'])\n",
    "years.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before we configure our first pipeline is to lowercase the tweets in `text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|robbie e responds to critics after win against eddie edwards in the #worldtitleseries https://t.co/nsybbmvjkz|\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "tweets = tweets.withColumn(\"text\",lower(col('text')))\n",
    "tweets.select(\"text\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1\n",
    "For our first pipeline we:\n",
    "- impute median values in `tweet_count`\n",
    "- convert `gender` to numeric type\n",
    "- tokenize and remove stop words in `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute median to null values in \"tweet_count\"\n",
    "imputer = Imputer(inputCols=[\"tweet_count\"], outputCols=[\"tweet_count_new\"]).setStrategy(\"median\")\n",
    "\n",
    "# convert \"gender\" to numeric type (0 = female, 1 = male, 2 = brand)\n",
    "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_num\")\n",
    "\n",
    "# process \"text\"\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") #tokenize\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\") #remove stop words\n",
    "\n",
    "# build pipeline\n",
    "pipeline = Pipeline(stages=[imputer, indexer, tokenizer, remover])\n",
    "\n",
    "# fit & tranform pipeline\n",
    "tweets = pipeline.fit(tweets).transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ML Feature library does not support stemming, we decide to stem the `text` column using the `SnowballStemmer` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|stemmed                                                                                 |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|[robbi, e, respond, critic, win, eddi, edward, #worldtitleseri, https://t.co/nsybbmvjkz]|\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "stemmed = tweets.withColumn(\"stemmed\", stemmer_udf(\"filtered\")).select(\"stemmed\")\n",
    "stemmed.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Words Used by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns to `tweets`\n",
    "We add `stemmed`,`punc`,`emojis`, and `account_years` to our updated dataset `tweets` from pipeline 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_new = tweets.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "punc_new = punc.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "emojis_new = emojis.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "years_new=years.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "stemmed_new = stemmed.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_new.join(punc_new, on=[\"row_index\"]).join(emojis_new, on=[\"row_index\"]) \\\n",
    "            .join(years_new, on=[\"row_index\"]).join(stemmed_new, on=[\"row_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_index',\n",
       " 'gender',\n",
       " 'created',\n",
       " 'fav_number',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_count',\n",
       " 'tweet_created',\n",
       " 'tweet_count_new',\n",
       " 'gender_num',\n",
       " 'words',\n",
       " 'filtered',\n",
       " 'punc',\n",
       " 'emojis',\n",
       " 'account_years',\n",
       " 'stemmed']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2\n",
    "For our second pipeline we:\n",
    "- compute term frequency vector for `stemmed`\n",
    "- create `bigrams` from `stemmed`\n",
    "- compute term frequency vector for `bigrams`\n",
    "- combine all relevant columns into `features` using VectorAssembler\n",
    "- scale `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "htf = HashingTF(inputCol=\"stemmed\", outputCol=\"tf\") #term frequencies for stemmed\n",
    "\n",
    "bigrams = NGram(n=2, inputCol=\"stemmed\", outputCol=\"bigrams\") #create bigrams\n",
    "bhtf = HashingTF(inputCol=\"bigrams\", outputCol=\"btf\") #term frequencies for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipeline = Pipeline(stages=[htf, bigrams, bhtf])\n",
    "\n",
    "# fit & tranform pipeline\n",
    "tweets = pipeline.fit(tweets).transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_index',\n",
       " 'gender',\n",
       " 'created',\n",
       " 'fav_number',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_count',\n",
       " 'tweet_created',\n",
       " 'tweet_count_new',\n",
       " 'gender_num',\n",
       " 'words',\n",
       " 'filtered',\n",
       " 'punc',\n",
       " 'emojis',\n",
       " 'account_years',\n",
       " 'stemmed',\n",
       " 'tf',\n",
       " 'bigrams',\n",
       " 'btf']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 91.0 failed 1 times, most recent failure: Lost task 0.0 in stage 91.0 (TID 668, jupyter-ishana-5fnarayanan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-42-6269eeb484ce>\", line 3, in <lambda>\nNameError: name 'DenseVector' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-42-6269eeb484ce>\", line 3, in <lambda>\nNameError: name 'DenseVector' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6269eeb484ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDenseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tf_dense'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonotonically_increasing_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    600\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    601\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \"\"\"\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \"\"\"\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 91.0 failed 1 times, most recent failure: Lost task 0.0 in stage 91.0 (TID 668, jupyter-ishana-5fnarayanan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-42-6269eeb484ce>\", line 3, in <lambda>\nNameError: name 'DenseVector' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-42-6269eeb484ce>\", line 3, in <lambda>\nNameError: name 'DenseVector' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "tf = tweets.select('tf').rdd.map(lambda x: DenseVector(x.toArray())).map(lambda x: (x, )).toDF(['tf_dense'])\n",
    "\n",
    "tf_df = tf.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#tweets_2 = tweets.join(tf_df, on=[\"row_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o495.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 82.0 failed 1 times, most recent failure: Lost task 0.0 in stage 82.0 (TID 660, jupyter-ishana-5fnarayanan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in toInternal\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in <genexpr>\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 444, in toInternal\n    return self.dataType.toInternal(obj)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 687, in toInternal\n    return self._cachedSqlType().toInternal(self.serialize(obj))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in serialize\n    values = [float(v) for v in obj]\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in <listcomp>\n    values = [float(v) for v in obj]\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in toInternal\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in <genexpr>\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 444, in toInternal\n    return self.dataType.toInternal(obj)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 687, in toInternal\n    return self._cachedSqlType().toInternal(self.serialize(obj))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in serialize\n    values = [float(v) for v in obj]\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in <listcomp>\n    values = [float(v) for v in obj]\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ed26ab065692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o495.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 82.0 failed 1 times, most recent failure: Lost task 0.0 in stage 82.0 (TID 660, jupyter-ishana-5fnarayanan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in toInternal\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in <genexpr>\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 444, in toInternal\n    return self.dataType.toInternal(obj)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 687, in toInternal\n    return self._cachedSqlType().toInternal(self.serialize(obj))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in serialize\n    values = [float(v) for v in obj]\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in <listcomp>\n    values = [float(v) for v in obj]\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in toInternal\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in <genexpr>\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 444, in toInternal\n    return self.dataType.toInternal(obj)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 687, in toInternal\n    return self._cachedSqlType().toInternal(self.serialize(obj))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in serialize\n    values = [float(v) for v in obj]\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in <listcomp>\n    values = [float(v) for v in obj]\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "tf_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on htf and bhtf\n",
    "normalizer = Normalizer(inputCol=\"tf_dense\", outputCol=\"normFeatures\", p=1.0)\n",
    "\n",
    "pca_1 = PCA(k=3, inputCol=\"normFeatures\", outputCol=\"pcaFeatures_1\")\n",
    "#pca_2 = PCA(k=3, inputCol = \"btf\", outputCol = \"pcaFeatures_2\")\n",
    "\n",
    "# create features column\n",
    "va = VectorAssembler(inputCols=[\"pcaFeatures_1\", \"punc\", \"emojis\", \"tweet_count_new\", \"fav_number\", \"retweet_count\",\n",
    "                               \"account_years\"], outputCol=\"features\")\n",
    "\n",
    "# scale features\n",
    "scaler1 = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "#scaler2 = StandardScaler(inputCol=\"pcaFeatures_1\", outputCol=\"scaledTF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o501.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 651, jupyter-ishana-5fnarayanan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in toInternal\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in <genexpr>\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 444, in toInternal\n    return self.dataType.toInternal(obj)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 687, in toInternal\n    return self._cachedSqlType().toInternal(self.serialize(obj))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in serialize\n    values = [float(v) for v in obj]\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in <listcomp>\n    values = [float(v) for v in obj]\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1437)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1437)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:44)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in toInternal\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in <genexpr>\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 444, in toInternal\n    return self.dataType.toInternal(obj)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 687, in toInternal\n    return self._cachedSqlType().toInternal(self.serialize(obj))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in serialize\n    values = [float(v) for v in obj]\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in <listcomp>\n    values = [float(v) for v in obj]\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c2107ed41be4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fit & tranform pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o501.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 651, jupyter-ishana-5fnarayanan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in toInternal\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in <genexpr>\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 444, in toInternal\n    return self.dataType.toInternal(obj)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 687, in toInternal\n    return self._cachedSqlType().toInternal(self.serialize(obj))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in serialize\n    values = [float(v) for v in obj]\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in <listcomp>\n    values = [float(v) for v in obj]\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1437)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1437)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:44)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in toInternal\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/types.py\", line 606, in <genexpr>\n    for f, v, c in zip(self.fields, obj, self._needConversion))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 444, in toInternal\n    return self.dataType.toInternal(obj)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 687, in toInternal\n    return self._cachedSqlType().toInternal(self.serialize(obj))\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in serialize\n    values = [float(v) for v in obj]\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 164, in <listcomp>\n    values = [float(v) for v in obj]\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# build pipeline\n",
    "pipeline_2 = Pipeline(stages=[normalizer, pca_1, va, scaler1])\n",
    "\n",
    "# fit & tranform pipeline\n",
    "final = pipeline_2.fit(tweets_2).transform(tweets_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model\n",
    "Our benchmark model is a multinomial logistic regression model with `scaledTF`, term frequency of tweets, as our only feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets.select(\"scaledTF\", \"gender_num\") \\\n",
    "        .withColumnRenamed('gender_num', 'label').withColumnRenamed('scaledTF', 'features')\n",
    "text.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "(bench_training, bench_test) = text.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estimator.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# 2-Fold Cross Validation w/ Logistic Regression estimator\n",
    "crossval = CrossValidator(estimator=estimator,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "cvModel = crossval.fit(bench_training)\n",
    "predictions = cvModel.transform(bench_test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with All Features\n",
    "Now, we will create 4 models using all our features (`tf`, `btf`, `punc`, `emojis`, `tweet_count_new`, `fav_number`, `retweet_count`, `account_years`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = tweets.select(\"scaledFeatures\", \"gender_num\") \\\n",
    "        .withColumnRenamed('gender_num', 'label').withColumnRenamed('scaledFeatures', 'features')\n",
    "final.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "(training, test) = final.randomSplit([0.7,0.3])\n",
    "print(\"Count of training set = \" + str(training.count()))\n",
    "print(\"Count of test set = \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estimator.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# 2-Fold Cross Validation w/ Logistic Regression estimator\n",
    "crossval = CrossValidator(estimator=estimator,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "predictions_lr = cvModel.transform(test)\n",
    "accuracy_lr = evaluator.evaluate(predictions_lr)\n",
    "print(\"Accuracy = \" + str(accuracy_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estimator.numTrees, [100, 250]) \\\n",
    "    .build()\n",
    "\n",
    "# 2-Fold Cross Validation w/ Random Forest estimator\n",
    "crossval = CrossValidator(estimator=estimator,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "predictions_rf = cvModel.transform(test)\n",
    "accuracy_rf = evaluator.evaluate(predictions_rf)\n",
    "print(\"Accuracy = \" + str(accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "estimator = NaiveBayes(modelType=\"multinomial\")\n",
    "\n",
    "# parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estimator.smoothing, [0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation w/ Naive Bayes estimator\n",
    "crossval = CrossValidator(estimator=estimator,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "predictions_nb = cvModel.transform(test)\n",
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "print(\"Accuracy = \" + str(accuracy_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "estimator = BisectingKMeans().setSeed(1)\n",
    "\n",
    "# parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estimator.setK, [3.0, 4.0, 5.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation w/ KMeans estimator\n",
    "crossval = CrossValidator(estimator=estimator,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=ClusteringEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "predictions_km = cvModel.transform(test)\n",
    "evaluator = ClusteringEvaluator()\n",
    "distance = evaluator.evaluate(predictions_km)\n",
    "print(\"Squared Euclidean Distance = \" + str(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multinomial Logistic Regression Accuracy = \" + str(accuracy_lr))\n",
    "print(\"Random Forest Accuracy = \" + str(accuracy_rf))\n",
    "print(\"Naive Bayes Accuracy = \" + str(accuracy_nb))\n",
    "print(\"KMeans Squared Euclidean Distance = \" + str(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
