{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Gender Classification\n",
    "### Final Project\n",
    "\n",
    "#### University of California, Santa Barbara\n",
    "#### PSTAT 135: Big Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/crowdflower/twitter-user-gender-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains about 20,000 rows, each with a user name, a random tweet, account profile and image, location, and link and sidebar color. All tweets were posted on October 26, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "tweets = spark.read.csv('gender_data.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24230"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unit_id',\n",
       " '_golden',\n",
       " '_unit_state',\n",
       " '_trusted_judgments',\n",
       " '_last_judgment_at',\n",
       " 'gender',\n",
       " 'gender:confidence',\n",
       " 'profile_yn',\n",
       " 'profile_yn:confidence',\n",
       " 'created',\n",
       " 'description',\n",
       " 'fav_number',\n",
       " 'gender_gold',\n",
       " 'link_color',\n",
       " 'name',\n",
       " 'profile_yn_gold',\n",
       " 'profileimage',\n",
       " 'retweet_count',\n",
       " 'sidebar_color',\n",
       " 'text',\n",
       " 'tweet_coord',\n",
       " 'tweet_count',\n",
       " 'tweet_created',\n",
       " 'tweet_id',\n",
       " 'tweet_location',\n",
       " 'user_timezone']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We removed columns `_golden`, `_unit_state`, `_last_judgment_at`, `gender:confidence`, `profile_yn`, `profile_yn:confidence`, `link_color`, `profile_yn_gold`, `profileimage`, and `sidebar_color` because they were not relevant in our model/purpose.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'created',\n",
       " 'description',\n",
       " 'fav_number',\n",
       " 'name',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_coord',\n",
       " 'tweet_count',\n",
       " 'tweet_created',\n",
       " 'tweet_id',\n",
       " 'tweet_location',\n",
       " 'user_timezone']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['_unit_id','_golden','_unit_state','_trusted_judgments','_last_judgment_at',\n",
    "                   'gender:confidence','profile_yn','profile_yn:confidence','gender_gold',\n",
    "                   'profile_yn_gold','link_color','profileimage','sidebar_color']\n",
    "tweets = tweets.drop(*columns_to_drop)\n",
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the `gender` column, we found that there were other labels besides `male`, `female`, and `brand`. Therefore, we are only going to keep rows where they have 1 of these 3 labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.filter((tweets.gender == 'male') | (tweets.gender == 'female') | (tweets.gender == 'brand'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18836"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From above, we see that after filtering gender, we have 18,836 rows. Next, we will check to see that each row has non-empty or non-null values in column `text`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17748"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of non-null values in \"text\"\n",
    "tweets.filter(tweets.text.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have 17,748 non-null values, so there is a presence of null values in this column. Thus, we will remove rows with null values, since actual text is important in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.filter(tweets.text.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For columns `fav_number`, `retweet_count`, and `tweet_count`, we will check to see if there are null values.** \n",
    "\n",
    "- If more than 10% of the values are null, we will not include that column later on in our model. \n",
    "\n",
    "- If less than 10% of the values are null, we will replace null values with the median of that specific column. We chose median because there is a possibility the mean could be something like 77.5, and you cannot retweet something 77.5 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fav_number` : Since there are no null values, there is no need to do any replacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of null values in \"fav_number\"\n",
    "tweets.filter(tweets.fav_number.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`retweet_count` : Since there are no null values, there is no need to do any replacing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of null values in \"retweet_count\"\n",
    "tweets.filter(tweets.retweet_count.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tweet_count` : There appears to be 1,259 null values. Since null values make up less than 10% of this column, we will replace these null values with the median of this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of null values in \"tweet_count\"\n",
    "tweets.filter(tweets.tweet_count.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied **Imputer** because it imputes missing values using mean (the default) or median in columns where missing values are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median of \"tweet_count\"\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tweets = tweets.withColumn(\"tweet_count\",tweets[\"tweet_count\"].cast(IntegerType()))\n",
    "imputer = Imputer(inputCols=[\"tweet_count\"], outputCols=[\"tweet_count_new\"]).setStrategy(\"median\")\n",
    "tweets = imputer.fit(tweets).transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check to see if there are any null values left in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.filter(tweets.tweet_count_new.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are interested in getting the number of years the twitter user has had there account, up to the date of the posted tweet we have in our data. We will create a row with this count of years and call it `account_years`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       created|\n",
      "+--------------+\n",
      "|  12/5/13 1:48|\n",
      "| 10/1/12 13:51|\n",
      "|11/28/14 11:30|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select('created').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use rdd, extract only the \"year\" part of the date, subtract from \"15\" (representing 2015)\n",
    "rdd = tweets.rdd\n",
    "years = rdd.map(lambda row: row['created'].split(' ')) \\\n",
    "                .map(lambda x: x[0]) \\\n",
    "                .map(lambda x: x.split('/')) \\\n",
    "                .map(lambda x: x[2]) \\\n",
    "                .map(lambda x: (15-int(x))) \\\n",
    "                .map(lambda x: (x, )).toDF(['account_years'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'account_years'>\n",
      "+------+-------------+--------------------+----------+-----------+-------------+--------------------+-----------+-----------+--------------+----------+---------------+--------------------+---------------+--------------------+--------------------+\n",
      "|gender|      created|         description|fav_number|       name|retweet_count|                text|tweet_coord|tweet_count| tweet_created|  tweet_id| tweet_location|       user_timezone|tweet_count_new|               words|            filtered|\n",
      "+------+-------------+--------------------+----------+-----------+-------------+--------------------+-----------+-----------+--------------+----------+---------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  male| 12/5/13 1:48|i sing my own rhy...|         0|    sheezy0|            0|robbie e responds...|       null|     110964|10/26/15 12:40|6.5873E+17|main; @Kan1shk3|             Chennai|         110964|[robbie, e, respo...|[robbie, e, respo...|\n",
      "|  male|10/1/12 13:51|I'm the author of...|        68|DavdBurnett|            0|���it felt like t...|       null|       7471|10/26/15 12:40|6.5873E+17|           null|Eastern Time (US ...|           7471|[���it, felt, lik...|[���it, felt, lik...|\n",
      "+------+-------------+--------------------+----------+-----------+-------------+--------------------+-----------+-----------+--------------+----------+---------------+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(years.account_years)\n",
    "tweets.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|account_years|\n",
      "+-------------+\n",
      "|            2|\n",
      "|            3|\n",
      "|            1|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert back to dataframe, add calculated years to \"tweets\" as \"account_years\"\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "tweets_new=tweets.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "years_new=years.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "tweets_years = tweets_new.join(years_new, on=[\"row_index\"]).drop(\"row_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+----------+--------------+-------------+--------------------+-----------+-----------+--------------+----------+---------------+--------------------+---------------+--------------------+--------------------+-------------+\n",
      "|gender|       created|         description|fav_number|          name|retweet_count|                text|tweet_coord|tweet_count| tweet_created|  tweet_id| tweet_location|       user_timezone|tweet_count_new|               words|            filtered|account_years|\n",
      "+------+--------------+--------------------+----------+--------------+-------------+--------------------+-----------+-----------+--------------+----------+---------------+--------------------+---------------+--------------------+--------------------+-------------+\n",
      "|  male|  12/5/13 1:48|i sing my own rhy...|         0|       sheezy0|            0|robbie e responds...|       null|     110964|10/26/15 12:40|6.5873E+17|main; @Kan1shk3|             Chennai|         110964|[robbie, e, respo...|[robbie, e, respo...|            2|\n",
      "|  male| 10/1/12 13:51|I'm the author of...|        68|   DavdBurnett|            0|���it felt like t...|       null|       7471|10/26/15 12:40|6.5873E+17|           null|Eastern Time (US ...|           7471|[���it, felt, lik...|[���it, felt, lik...|            3|\n",
      "|  male|11/28/14 11:30|louis whining and...|      7696|lwtprettylaugh|            1|i absolutely ador...|       null|       5617|10/26/15 12:40|6.5873E+17|         clcncl|            Belgrade|           5617|[i, absolutely, a...|[absolutely, ador...|            1|\n",
      "+------+--------------+--------------------+----------+--------------+-------------+--------------------+-----------+-----------+--------------+----------+---------------+--------------------+---------------+--------------------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets_years\n",
    "tweets.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the counts and frequencies, average number of favorites (`fav_number`), average number of retweets (`retween_count`), average number of tweets (`tweet_count`), and average number of account years for each `gender` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|female| 6354|\n",
      "|  male| 5776|\n",
      "| brand| 5618|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# counts\n",
    "tweets.groupBy('gender').count().orderBy('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"account_years\" among (gender, created, description, fav_number, name, retweet_count, text, tweet_coord, tweet_count, tweet_created, tweet_id, tweet_location, user_timezone, tweet_count_new);",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-596eee845120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# brand averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgender\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"brand\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fav_number'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'retweet_count'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet_count'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'account_years'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m         \"\"\"\n\u001b[0;32m-> 1546\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exprs should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"account_years\" among (gender, created, description, fav_number, name, retweet_count, text, tweet_coord, tweet_count, tweet_created, tweet_id, tweet_location, user_timezone, tweet_count_new);"
     ]
    }
   ],
   "source": [
    "# brand averages\n",
    "tweets.filter(tweets.gender == \"brand\") \\\n",
    "            .agg({'fav_number':'avg','retweet_count':'avg','tweet_count':'avg', 'account_years':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# female averages\n",
    "tweets.filter(tweets.gender == \"female\") \\\n",
    "            .agg({'fav_number':'avg','retweet_count':'avg','tweet_count':'avg', 'account_years':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male averages\n",
    "tweets.filter(tweets.gender == \"male\") \\\n",
    "            .agg({'fav_number':'avg','retweet_count':'avg','tweet_count':'avg', 'account_years':'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We derived features based on casing, punctuation, and emojis on the tweets in the `text` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Casing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {False: 252068, True: 12995})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts of Lowercase (True), Uppercase (False)\n",
    "\n",
    "words = rdd.flatMap(lambda row: row['text'].split(' '))\n",
    "wordcounts = words.map(lambda x: (x, 0)) \\\n",
    "                    .map(lambda x: x[0].isupper())\n",
    "wordcounts.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04902608059216111"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12995/(12995+252068)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12,995 upper-case words in the `text` column. As you can see above, upper-case words make up less than 5% of the total words, so we do not think it is a heavily-weighted factor in deciding gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[': 96,\n",
       " '(': 18409,\n",
       " '=': 17774,\n",
       " \"'\": 32906,\n",
       " '#': 5809,\n",
       " ':': 11900,\n",
       " '/': 21656,\n",
       " '.': 21515,\n",
       " ')': 18566,\n",
       " ',': 22674,\n",
       " '\\\\': 430,\n",
       " '\"': 13310,\n",
       " '@': 7858,\n",
       " '-': 2266,\n",
       " '?': 1707,\n",
       " '!': 3546,\n",
       " '+': 129,\n",
       " '_': 7190,\n",
       " '%': 106,\n",
       " '&': 679,\n",
       " ';': 811,\n",
       " ']': 95,\n",
       " '|': 132,\n",
       " '$': 205,\n",
       " '~': 52,\n",
       " '*': 291,\n",
       " '^': 26,\n",
       " '{': 11,\n",
       " '}': 8,\n",
       " '`': 5}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count punctuation (!, ?, .)\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(str(tweets.select('text').take(17748)))\n",
    "punctuation_counts = {k:v for k, v in counts.items() if k in punctuation}\n",
    "punctuation_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Emojis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, emojis are represented by the character '�', so we will see if any of these characters are present in each row tweet. We will add a column titled `emojis`, with the value 0=False (no emojis present) and 1=True (emojis present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Emojis\n",
    "emojis_TF = tweets.select(\"text\").rdd.map(lambda tweet: str(tweet)).map(lambda x: '�' in x)\n",
    "emojis_TF.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will process the tweets. We can monitor our changes by looking at row 1 of `text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Casing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|Robbie E Responds To Critics After Win Against Eddie Edwards In The #WorldTitleSeries https://t.co/NSybBmVjKZ|\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select(\"text\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|robbie e responds to critics after win against eddie edwards in the #worldtitleseries https://t.co/nsybbmvjkz|\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lowercase words\n",
    "from pyspark.sql.functions import lower, col\n",
    "tweets = tweets.withColumn(\"text\",lower(col('text')))\n",
    "tweets.select(\"text\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|[robbie, e, responds, to, critics, after, win, against, eddie, edwards, in, the, #worldtitleseries, https://t.co/nsybbmvjkz]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tweets = tokenizer.transform(tweets)\n",
    "tweets.select(\"words\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop Word Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------+\n",
      "|filtered                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "|[robbie, e, responds, critics, win, eddie, edwards, #worldtitleseries, https://t.co/nsybbmvjkz]|\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stop Word Removal\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "tweets = remover.transform(tweets)\n",
    "tweets.select(\"filtered\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "stem_tweets = tweets.withColumn(\"stemmed\", stemmer_udf(\"filtered\")).select(\"stemmed\")\n",
    "stem_tweets.select(\"stemmed\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|bigrams                                                                                                                                 |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[robbi e, e respond, respond critic, critic win, win eddi, eddi edward, edward #worldtitleseri, #worldtitleseri https://t.co/nsybbmvjkz]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "bigram = NGram(n=2, inputCol=\"stemmed\", outputCol=\"bigrams\")\n",
    "bigram_tweets = bigram.transform(stem_tweets)\n",
    "bigram_tweets.select(\"bigrams\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(10000, {162: 1.0, 1521: 1.0, 1785: 1.0, 4032: 1.0, 5505: 1.0, 5993: 1.0, 6493: 1.0, 7437: 1.0, 9022: 1.0})]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "tf = HashingTF(numFeatures=10000)\n",
    "bigram_features = bigram_tweets.rdd.map(lambda tweet: tf.transform(tweet[0]))\n",
    "bigram_features.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will apply Vector Assembler to prep the data for various ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [[Counts for each word], [Bigrams], [Punctuation counts, ** Emoji Counts **], tweets counts, favorited tweets, retweet counts, account years]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HashingTF for stemmed tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "tf = HashingTF(numFeatures=10000)\n",
    "stem_features = stem_tweets.rdd.map(lambda tweet: tf.transform(tweet[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(10000, {162: 1.0, 1521: 1.0, 1785: 1.0, 4032: 1.0, 5505: 1.0, 5993: 1.0, 6493: 1.0, 7437: 1.0, 9022: 1.0}),\n",
       " SparseVector(10000, {213: 1.0, 1065: 1.0, 1074: 1.0, 1136: 1.0, 2547: 1.0, 4623: 1.0, 6992: 1.0, 7449: 1.0, 8194: 1.0, 8854: 1.0, 9546: 1.0})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_features.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding columns for counts of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, True, True]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def punc_func(a):\n",
    "    if a in punctuation:\n",
    "        return true\n",
    "    return false\n",
    "\n",
    "punc = tweets.select(\"text\").rdd.map(lambda tweet: Counter(str(tweet))).map(lambda x: '' in x)\n",
    "punc.take(5)\n",
    "#punc_2 = punc.map(lambda x: list(x.keys())).map(lambda y: 1 if y[0] in punctuation else None)\n",
    "#punc_2 = punc.map(lambda x: x)\n",
    "#punc_2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#assembler = VectorAssembler(\n",
    "#   inputCols=[\"text\"],\n",
    "#    outputCol=\"features\")\n",
    "\n",
    "#output = assembler.transform(tweets)\n",
    "#output.select(\"features\", \"text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = tweets.select(\"text\").rdd.map(lambda tweet: Counter(str(tweet))).map(lambda x: ':' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
