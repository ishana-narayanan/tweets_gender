{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Gender Classification\n",
    "### Final Project\n",
    "\n",
    "#### University of California, Santa Barbara\n",
    "#### PSTAT 135: Big Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/crowdflower/twitter-user-gender-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains about 20,000 rows, each with a user name, a random tweet, profile image and statistics, location, and link and sidebar color. All tweets were posted in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "tweets = spark.read.csv('gender_data.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24230"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unit_id',\n",
       " '_golden',\n",
       " '_unit_state',\n",
       " '_trusted_judgments',\n",
       " '_last_judgment_at',\n",
       " 'gender',\n",
       " 'gender:confidence',\n",
       " 'profile_yn',\n",
       " 'profile_yn:confidence',\n",
       " 'created',\n",
       " 'description',\n",
       " 'fav_number',\n",
       " 'gender_gold',\n",
       " 'link_color',\n",
       " 'name',\n",
       " 'profile_yn_gold',\n",
       " 'profileimage',\n",
       " 'retweet_count',\n",
       " 'sidebar_color',\n",
       " 'text',\n",
       " 'tweet_coord',\n",
       " 'tweet_count',\n",
       " 'tweet_created',\n",
       " 'tweet_id',\n",
       " 'tweet_location',\n",
       " 'user_timezone']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove columns `_unit_id`, `_golden`, `_unit_state`, `_last_judgment_at`,`_trusted_judgments`, `gender:confidence`, `profile_yn`, `profile_yn:confidence`, `description`, `gender_gold`, `link_color`,`name`, `profile_yn_gold`, `profileimage`, `sidebar_color`, `tweet_coord`, `tweet_id`, `tweet_location`, and `user_timezone` because they were not relevant in our model/purpose...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+-------------+--------------------+-----------+--------------+\n",
      "|gender|       created|fav_number|retweet_count|                text|tweet_count| tweet_created|\n",
      "+------+--------------+----------+-------------+--------------------+-----------+--------------+\n",
      "|  male|  12/5/13 1:48|         0|            0|Robbie E Responds...|     110964|10/26/15 12:40|\n",
      "|  male| 10/1/12 13:51|        68|            0|���It felt like t...|       7471|10/26/15 12:40|\n",
      "|  male|11/28/14 11:30|      7696|            1|i absolutely ador...|       5617|10/26/15 12:40|\n",
      "|  male| 6/11/09 22:39|       202|            0|Hi @JordanSpieth ...|       1693|10/26/15 12:40|\n",
      "|female| 4/16/14 13:23|     37318|            0|Watching Neighbou...|      31462|10/26/15 12:40|\n",
      "+------+--------------+----------+-------------+--------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.select(['gender','created','fav_number','retweet_count','text','tweet_count','tweet_created'])\n",
    "tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              gender|count|\n",
      "+--------------------+-----+\n",
      "|              female| 6700|\n",
      "|                male| 6194|\n",
      "|               brand| 5942|\n",
      "|                null| 3188|\n",
      "|             unknown| 1117|\n",
      "|          6.5874E+17|   83|\n",
      "|          6.5873E+17|   62|\n",
      "|              Medina|   14|\n",
      "|              London|   13|\n",
      "|      10/26/15 12:40|   11|\n",
      "|Porto, Portugal �...|   10|\n",
      "|1/16 cute pickle ...|   10|\n",
      "|              0084B4|    9|\n",
      "|Republic of the P...|    8|\n",
      "|                  UK|    7|\n",
      "|                 USA|    7|\n",
      "|         3/4 + band |    6|\n",
      "|              FF005E|    6|\n",
      "|PRE-ORDER MADE IN...|    5|\n",
      "|           New York |    4|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.groupBy('gender').count().sort('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the `gender` column contains many labels. We decide to keep only the tweets made by a `male`, `female`, or `brand` since they are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18836"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.filter((tweets.gender == 'male') | (tweets.gender == 'female') | (tweets.gender == 'brand'))\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove any null values in `text` since we are only interested in predicting gender for users with tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of null values in \"text\"\n",
    "tweets.filter(tweets.text.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17748"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.filter(tweets.text.isNotNull())\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns `fav_number`, `retweet_count`, and `tweet_count`:\n",
    "\n",
    "- If more than 10% of the values are null, we will not include that column later on in our model. \n",
    "\n",
    "- If less than 10% of the values are null, we will replace null values with the median of that specific column. We chose median because there is a possibility the mean could be something like 77.5, and you cannot retweet something 77.5 times.\n",
    "\n",
    "- If there are no null values, we will keep the column unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of null values in \"fav_number\"\n",
    "tweets.filter(tweets.fav_number.isNull()).count()/tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of null values in \"retweet_count\"\n",
    "tweets.filter(tweets.retweet_count.isNull()).count()/tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07093757043047104"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of null values in \"tweet_count\"\n",
    "tweets.filter(tweets.tweet_count.isNull()).count()/tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since less than 10% of `tweet_count` values are null, we will replace them with the median value of `tweet_count` during pipeline building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Columns to Type Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.withColumn(\"fav_number\",tweets[\"fav_number\"].cast('integer'))\n",
    "tweets = tweets.withColumn(\"retweet_count\",tweets[\"retweet_count\"].cast('integer'))\n",
    "tweets = tweets.withColumn(\"tweet_count\",tweets[\"tweet_count\"].cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Columns for Punctuation Counts, Emoji Existence, and Account Years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting punctuation counts for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "def punc_func(tweet):\n",
    "    sum = 0\n",
    "    for key in tweet:\n",
    "        if key in punctuation:\n",
    "            sum += tweet[key]\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|punc|\n",
      "+----+\n",
      "|  11|\n",
      "|  18|\n",
      "|   5|\n",
      "|  19|\n",
      "|  15|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "punc = tweets.select(\"text\").rdd.map(lambda tweet: Counter(str(tweet))) \\\n",
    "            .map(lambda x: punc_func(x)).map(lambda x: (x, )).toDF(['punc'])\n",
    "punc.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting boolean values if an emoji, represented by the character '�', is present in a tweet (True) or not present in a tweet (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|emojis|\n",
      "+------+\n",
      "| false|\n",
      "|  true|\n",
      "| false|\n",
      "| false|\n",
      "|  true|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emojis = tweets.select(\"text\").rdd.map(lambda tweet: str(tweet)) \\\n",
    "            .map(lambda x: '�' in x).map(lambda x: (x, )).toDF(['emojis'])\n",
    "emojis.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of years each Twitter user has had their account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|account_years|\n",
      "+-------------+\n",
      "|            2|\n",
      "|            3|\n",
      "|            1|\n",
      "|            6|\n",
      "|            1|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extracting only the \"year\" part of 'created' & subtracting it from 15 (representing 2015)\n",
    "years = tweets.rdd.map(lambda row: row['created'].split(' ')) \\\n",
    "                .map(lambda x: x[0]) \\\n",
    "                .map(lambda x: x.split('/')) \\\n",
    "                .map(lambda x: x[2]) \\\n",
    "                .map(lambda x: (15-int(x))) \\\n",
    "                .map(lambda x: (x, )).toDF(['account_years'])\n",
    "years.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before we configure our first pipeline is to lowercase the tweets in `text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|robbie e responds to critics after win against eddie edwards in the #worldtitleseries https://t.co/nsybbmvjkz|\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "tweets = tweets.withColumn(\"text\",lower(col('text')))\n",
    "tweets.select(\"text\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1\n",
    "For our first pipeline we:\n",
    "- impute median values in `tweet_count`\n",
    "- convert `gender` to numeric type\n",
    "- tokenize and remove stop words in `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute median to null values in \"tweet_count\"\n",
    "imputer = Imputer(inputCols=[\"tweet_count\"], outputCols=[\"tweet_count_new\"]).setStrategy(\"median\")\n",
    "\n",
    "# convert \"gender\" to numeric type (0 = female, 1 = male, 2 = brand)\n",
    "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_num\")\n",
    "\n",
    "# process \"text\"\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") #tokenize\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\") #remove stop words\n",
    "\n",
    "# build pipeline\n",
    "pipeline = Pipeline(stages=[imputer, indexer, tokenizer, remover])\n",
    "\n",
    "# fit & tranform pipeline\n",
    "tweets = pipeline.fit(tweets).transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ML Feature library does not support stemming, we decide to stem the `text` column using the `SnowballStemmer` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|stemmed                                                                                 |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|[robbi, e, respond, critic, win, eddi, edward, #worldtitleseri, https://t.co/nsybbmvjkz]|\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "stemmed = tweets.withColumn(\"stemmed\", stemmer_udf(\"filtered\")).select(\"stemmed\")\n",
    "stemmed.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns to `tweets`\n",
    "We add `stemmed`,`punc`,`emojis`, and `account_years` to our updated dataset `tweets` from pipeline 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_new = tweets.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "punc_new = punc.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "emojis_new = emojis.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "years_new=years.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "stemmed_new = stemmed.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_new.join(punc_new, on=[\"row_index\"]).join(emojis_new, on=[\"row_index\"]) \\\n",
    "            .join(years_new, on=[\"row_index\"]).join(stemmed_new, on=[\"row_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_index',\n",
       " 'gender',\n",
       " 'created',\n",
       " 'fav_number',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_count',\n",
       " 'tweet_created',\n",
       " 'tweet_count_new',\n",
       " 'gender_num',\n",
       " 'words',\n",
       " 'filtered',\n",
       " 'punc',\n",
       " 'emojis',\n",
       " 'account_years',\n",
       " 'stemmed']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2\n",
    "For our second pipeline we:\n",
    "- compute term frequency vector for `stemmed`\n",
    "- create `bigrams` from `stemmed`\n",
    "- compute term frequency vector for `bigrams`\n",
    "- combine all relevant columns into `features` using VectorAssembler\n",
    "- scale `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "htf = HashingTF(inputCol=\"stemmed\", outputCol=\"tf\", numFeatures=10000) #term frequencies for stemmed\n",
    "\n",
    "bigrams = NGram(n=2, inputCol=\"stemmed\", outputCol=\"bigrams\") #create bigrams\n",
    "bhtf = HashingTF(inputCol=\"bigrams\", outputCol=\"btf\", numFeatures=10000) #term frequencies for bigrams\n",
    "\n",
    "# create features column\n",
    "va = VectorAssembler(inputCols=[\"tf\", \"btf\", \"punc\", \"emojis\", \"tweet_count_new\", \"fav_number\", \"retweet_count\",\n",
    "                               \"account_years\"], outputCol=\"features\")\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# build pipeline\n",
    "pipeline = Pipeline(stages=[htf, bigrams, bhtf, va, scaler])\n",
    "\n",
    "# fit & tranform pipeline\n",
    "tweets = pipeline.fit(tweets).transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_index',\n",
       " 'gender',\n",
       " 'created',\n",
       " 'fav_number',\n",
       " 'retweet_count',\n",
       " 'text',\n",
       " 'tweet_count',\n",
       " 'tweet_created',\n",
       " 'tweet_count_new',\n",
       " 'gender_num',\n",
       " 'words',\n",
       " 'filtered',\n",
       " 'punc',\n",
       " 'emojis',\n",
       " 'account_years',\n",
       " 'stemmed',\n",
       " 'tf',\n",
       " 'bigrams',\n",
       " 'btf',\n",
       " 'features',\n",
       " 'scaledFeatures']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Dataframe\n",
    "For modeling, we will be using only the `scaledFeatures`, renamed as `features`, and `gender_num`, renamed as `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: double]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = tweets.select(\"scaledFeatures\", \"gender_num\") \\\n",
    "        .withColumnRenamed('gender_num', 'label').withColumnRenamed('scaledFeatures', 'features')\n",
    "final.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(20006,[2398,2500...|  1.0|\n",
      "|(20006,[472,488,4...|  1.0|\n",
      "|(20006,[2196,2996...|  1.0|\n",
      "|(20006,[62,375,11...|  1.0|\n",
      "|(20006,[1927,2708...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split `final` dataset into 70% training and 30% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of training set = 12522\n",
      "Count of test set = 5226\n"
     ]
    }
   ],
   "source": [
    "(training, test) = final.randomSplit([0.7,0.3])\n",
    "print(\"Count of training set = \" + str(training.count()))\n",
    "print(\"Count of test set = \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model = lr.fit(training)\n",
    "predictions_lr = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.36203597397627246\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator.evaluate(predictions_lr)\n",
    "print(\"Accuracy = \" + str(accuracy_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "model = rf.fit(training)\n",
    "predictions_rf = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.4393417527745886\n"
     ]
    }
   ],
   "source": [
    "accuracy_rf = evaluator.evaluate(predictions_rf)\n",
    "print(\"Accuracy = \" + str(accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(training)\n",
    "predictions_nb = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.4364714887102947\n"
     ]
    }
   ],
   "source": [
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "print(\"Accuracy = \" + str(accuracy_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model = kmeans.fit(training)\n",
    "predictions_km = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions_km)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
